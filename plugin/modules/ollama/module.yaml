name: ollama
description: Ollama local LLM backend
provides_services: [llm]
requires: [config, events]

config:
  endpoint:
    type: string
    default: "http://localhost:11434"
    widget: text
    label: Ollama URL
    public: true

  model:
    type: string
    default: ""
    widget: text
    label: Model
    placeholder: "llama3.2"
    public: true

  temperature:
    type: float
    default: 0.7
    min: 0.0
    max: 2.0
    step: 0.1
    widget: slider
    label: Temperature

  max_tokens:
    type: int
    default: 4096
    min: 1
    max: 128000
    widget: number
    label: Max Tokens

  request_timeout:
    type: int
    default: 300
    min: 10
    max: 600
    widget: number
    label: Request Timeout (seconds)
    description: Ollama may need longer for initial model loading
